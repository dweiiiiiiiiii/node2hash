{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import LinearSVC\n",
    "from utils import *\n",
    "#import argparse\n",
    "from datasets import *\n",
    "\n",
    "##################################################################################################\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-g\", \"--gpunum\", help=\"GPU number to train the model.\")\n",
    "# parser.add_argument(\"-d\", \"--dataset\", help=\"Name of the dataset.\")\n",
    "\n",
    "# ##################################################################################################\n",
    "# args = parser.parse_args()\n",
    "# if not args.gpunum:\n",
    "#     parser.error(\"Need to provide the GPU number.\")\n",
    "    \n",
    "# if not args.dataset:\n",
    "#     parser.error(\"Need to provide the dataset.\")\n",
    "        \n",
    "gpunum = \"2\"\n",
    "\n",
    "##################################################################################################\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=gpunum\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "dataset_name = \"cora\"\n",
    "#dataset_name = args.dataset\n",
    "if dataset_name in ['reuters', 'tmc', 'rcv1', 'dblp']:\n",
    "    single_label = False\n",
    "else:\n",
    "    single_label = True\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "max_nodes = 20\n",
    "\n",
    "# fn = os.path.join(data_dir, 'train.NN.pkl')\n",
    "# df_train = pd.read_pickle(fn)\n",
    "# #df_train.set_index('doc_id', inplace=True)\n",
    "\n",
    "# docid2index = {docid: index for index, docid in enumerate(list(df_train.index))}\n",
    "\n",
    "# # Test data\n",
    "# fn = os.path.join(data_dir, 'test.NN.pkl')\n",
    "# df_test = pd.read_pickle(fn)\n",
    "# #df_test.set_index('doc_id', inplace=True)\n",
    "\n",
    "data_dir = os.path.join('dataset/clean', dataset_name)\n",
    "\n",
    "train_set = TextDataset(dataset_name, data_dir, subset='train')\n",
    "test_set = TextDataset(dataset_name, data_dir, subset='test')\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=128, shuffle=True)\n",
    "\n",
    "num_train = len(train_set)\n",
    "num_test = len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "\n",
    "class MedianHashing(object):\n",
    "    def __init__(self):\n",
    "        self.threshold = None\n",
    "        self.latent_dim = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.threshold = np.median(X, axis=0)\n",
    "        self.latent_dim = X.shape[1]\n",
    "        \n",
    "    def transform(self, X):\n",
    "        assert(X.shape[1] == self.latent_dim)\n",
    "        binary_code = np.zeros(X.shape)\n",
    "        for i in range(self.latent_dim):\n",
    "            binary_code[np.nonzero(X[:,i] < self.threshold[i]),i] = 0\n",
    "            binary_code[np.nonzero(X[:,i] >= self.threshold[i]),i] = 1\n",
    "        return binary_code.astype(int)\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "class STH:\n",
    "    def __init__(self, num_bits, topK):\n",
    "        super(STH, self).__init__()\n",
    "        self.num_bits = num_bits\n",
    "        self.clfs = [LinearSVC() for n in range(num_bits)]\n",
    "        self.topK = topK\n",
    "    \n",
    "    def fit_transform(self, bow_mat, weight_mat, num_train):\n",
    "        W = weight_mat\n",
    "        D = np.asarray(W.sum(axis=1)).squeeze() + 0.0001 # adding damping value for a numerical stabability\n",
    "        D = scipy.sparse.diags(D)\n",
    "        L = D - W\n",
    "    \n",
    "        L = scipy.sparse.csc_matrix(L)\n",
    "        D = scipy.sparse.csc_matrix(D)\n",
    "\n",
    "        num_attempts = 0\n",
    "        max_attempts = 3\n",
    "        success = False\n",
    "        \n",
    "        while not success:\n",
    "            E, Y = eigsh(L, k=self.num_bits+1, M=D, which='SM')\n",
    "            success = np.all(np.isreal(Y))\n",
    "            \n",
    "            if not success:\n",
    "                print(\"Warning: Some eigenvalues are not real values. Retry to solve Eigen-decomposition.\")\n",
    "                num_attempts += 1\n",
    "            \n",
    "            if num_attempts > max_attempts:\n",
    "                assert(np.all(np.isreal(Y))) # if this fails, re-run fit again\n",
    "                assert(False) # Check your data \n",
    "        \n",
    "        Y = np.real(Y)\n",
    "        Y = Y[:, 1:]\n",
    "        \n",
    "        medHash = MedianHashing()\n",
    "        cbTrain = medHash.fit_transform(Y)    \n",
    "        for b in range(0, cbTrain.shape[1]):\n",
    "            self.clfs[b].fit(bow_mat, cbTrain[:, b])\n",
    "        return cbTrain\n",
    "    \n",
    "    def transform(self, bow_mat, num_test):\n",
    "        cbTest = np.zeros((num_test, self.num_bits), dtype=np.int64)\n",
    "        for b in range(0, self.num_bits):\n",
    "            cbTest[:,b] = self.clfs[b].predict(bow_mat)\n",
    "        return cbTest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docid2index = {docid: index for index, docid in enumerate(list(train_set.df.index))}\n",
    "r = []\n",
    "c = []\n",
    "row_index = 0\n",
    "for idx, row in train_set.df.iterrows():\n",
    "    col = [docid2index[docid] for docid in train_set.df.neighbors.iloc[idx].nonzero()[1]]\n",
    "    r += [row_index] * len(col)\n",
    "    c += col\n",
    "    row_index += 1\n",
    "\n",
    "d = [0.9] * len(c)\n",
    "weight_mat = csc_matrix((d, (r, c)), shape=(num_train, num_train))\n",
    "train_bow = sparse.vstack(list(train_set.df.bow))\n",
    "#test_bow = sparse.vstack(list(test_set.df.bow))\n",
    "   \n",
    "weight_mat = csc_matrix((d, (r, c)), shape=(num_train, num_train))\n",
    "train_bow = sparse.vstack(list(train_set.df.bow))\n",
    "test_bow = sparse.vstack(list(test_set.df.bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_mat[0].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.df.neighbors.iloc[0].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = torch.from_numpy(sparse.vstack(list(train_set.df.label)).toarray())\n",
    "test_y = torch.from_numpy(sparse.vstack(list(test_set.df.label)).toarray())\n",
    "assert(train_y.size(1) == test_y.size(1))\n",
    "    \n",
    "with torch.no_grad():\n",
    "    prec_results = []\n",
    "    \n",
    "    for num_bits in [16, 32, 64, 128]:\n",
    "        print('train STH with {} bits ...'.format(num_bits))\n",
    "        model = STH(num_bits, None)\n",
    "        train_b = model.fit_transform(train_bow, weight_mat, None)\n",
    "        test_b = model.transform(test_bow, test_bow.shape[0])\n",
    "\n",
    "        # convert hash to Tensor\n",
    "        train_b = torch.Tensor(list(train_b)).type(torch.ByteTensor)\n",
    "        test_b = torch.Tensor(list(test_b)).type(torch.ByteTensor)\n",
    "\n",
    "        assert(train_b.size(0) == train_y.size(0))\n",
    "        assert(test_b.size(0) == test_y.size(0))\n",
    "        assert(train_b.size(1) == test_b.size(1))\n",
    "\n",
    "        print(\"Evaluating the binary codes ...\")\n",
    "        retrieved_indices = retrieve_topk(test_b.to(device), train_b.to(device), topK=100)\n",
    "        prec = compute_precision_at_k(retrieved_indices, test_y.to(device), train_y.to(device), topK=100)\n",
    "\n",
    "        print(\"bit:{} precision at 100: {:.4f}\".format(num_bits, prec.item()))\n",
    "        prec_results.append(prec.item())\n",
    "\n",
    "        del train_b\n",
    "        del test_b\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    result = ' & '.join(['{:.4f}'.format(p) for p in prec_results])\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research2018",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
