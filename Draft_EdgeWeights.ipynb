{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk type: BFS with maximum nodes of: 20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "from model.EdgeReg import *\n",
    "import argparse\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-g\", \"--gpunum\", help=\"GPU number to train the model.\")\n",
    "# parser.add_argument(\"-d\", \"--dataset\", help=\"Name of the dataset.\")\n",
    "# parser.add_argument(\"-b\", \"--nbits\", help=\"Number of bits of the embedded vector.\", type=int)\n",
    "# parser.add_argument(\"-w\", \"--walk\", default=\"Immedidate-1\", help=\"Graph traversal strategy (BFS, DFS, Random), followed the maximum neighbors. E.g. BFS-20 we perform BFS upto 20 nodes.\")\n",
    "# parser.add_argument(\"--edge_weight\", default=1.0, type=float)\n",
    "# parser.add_argument(\"--dropout\", help=\"Dropout probability (0 means no dropout)\", default=0.1, type=float)\n",
    "# parser.add_argument(\"--train_batch_size\", default=100, type=int)\n",
    "# parser.add_argument(\"--test_batch_size\", default=100, type=int)\n",
    "# parser.add_argument(\"--transform_batch_size\", default=100, type=int)\n",
    "# parser.add_argument(\"-e\", \"--num_epochs\", default=30, type=int)\n",
    "# parser.add_argument(\"-T\", \"--num_samples\", default=1, type=int, help=\"number of samples from Q(z|x).\")\n",
    "# parser.add_argument(\"--lr\", default=0.001, type=float)\n",
    "# parser.add_argument(\"--topn\", default=20, type=int)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# if not args.gpunum:\n",
    "#     parser.error(\"Need to provide the GPU number.\")\n",
    "    \n",
    "# if not args.dataset:\n",
    "#     parser.error(\"Need to provide the dataset.\")\n",
    "\n",
    "# if not args.nbits:\n",
    "#     parser.error(\"Need to provide the number of bits.\")\n",
    "        \n",
    "# if not args.walk:\n",
    "#     parser.error(\"Need to provide the graph traversal method.\")\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "#######################################################################################################\n",
    "class TextAndNearestNeighborsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_name, data_dir, subset='train'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): Directory for loading and saving train, test, and cv dataframes.\n",
    "            subset (string): Specify subset of the datasets. The choices are: train, test, cv.\n",
    "        \"\"\"\n",
    "        self.dataset_name = dataset_name\n",
    "        self.data_dir = os.path.join(data_dir, dataset_name)\n",
    "        self.subset = subset\n",
    "        fn = '{}.{}.pkl'.format(dataset_name, subset)\n",
    "        self.df = self.load_df(self.data_dir, fn)\n",
    "        self.docid2index = {docid: index for index, docid in enumerate(list(self.df.index))}\n",
    "        \n",
    "        if dataset_name in ['reuters', 'rcv1', 'tmc']:\n",
    "            self.single_label = False\n",
    "        else:\n",
    "            self.single_label = True\n",
    "\n",
    "    def load_df(self, data_dir, df_file):\n",
    "        df_file = os.path.join(data_dir, df_file)\n",
    "        return pd.read_pickle(df_file)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        doc_id = self.df.iloc[idx].name\n",
    "        doc_bow = self.df.iloc[idx].bow\n",
    "        doc_bow = torch.from_numpy(doc_bow.toarray().squeeze().astype(np.float32))\n",
    "        \n",
    "        label = self.df.iloc[idx].label\n",
    "        label = torch.from_numpy(label.toarray().squeeze().astype(np.float32))\n",
    "                \n",
    "        neighbors = torch.LongTensor(self.df.iloc[idx].neighbors)\n",
    "        scores = torch.Tensor(self.df.iloc[idx].scores)\n",
    "        return (doc_id, doc_bow, label, neighbors, scores)\n",
    "    \n",
    "    def num_classes(self):\n",
    "        return self.df.iloc[0].label.shape[1]\n",
    "    \n",
    "    def num_features(self):\n",
    "        return self.df.iloc[0].bow.shape[1]\n",
    "    \n",
    "#######################################################################################################\n",
    "def BFS_walk(df, start_node_id, num_steps, max_branch_factor=20):\n",
    "    if isinstance(start_node_id, list):\n",
    "        queue = list(start_node_id)\n",
    "    else:\n",
    "        queue = [start_node_id]\n",
    "        \n",
    "    visited_nodes = set()\n",
    "    curr_step = 0\n",
    "    while len(queue) > 0:         \n",
    "        curr_node_id = queue.pop(0)\n",
    "        while curr_node_id in visited_nodes:\n",
    "            if len(queue) <= 0:\n",
    "                #if not isinstance(start_node_id, list):\n",
    "                #    visited_nodes.remove(start_node_id)\n",
    "                return list(visited_nodes)\n",
    "            curr_node_id = queue.pop(0)\n",
    "        \n",
    "        nn_list = list(train_set.df.loc[curr_node_id].neighbors[:max_branch_factor])\n",
    "        #np.random.shuffle(nn_list)\n",
    "        queue += nn_list\n",
    "        visited_nodes.add(curr_node_id)\n",
    "        curr_step += 1\n",
    "        if curr_step > num_steps:\n",
    "            break\n",
    "    \n",
    "    #if not isinstance(start_node_id, list):\n",
    "    #    visited_nodes.remove(start_node_id)\n",
    "    return list(visited_nodes)    \n",
    "\n",
    "#######################################################################################################\n",
    "walk_type, max_nodes = 'BFS-20'.split('-')\n",
    "max_nodes = int(max_nodes)\n",
    "print(\"Walk type: {} with maximum nodes of: {}\".format(walk_type, max_nodes))\n",
    "\n",
    "if walk_type == 'BFS':\n",
    "    neighbor_sample_func = BFS_walk\n",
    "elif walk_type == 'DFS':\n",
    "    neighbor_sample_func = DFS_walk\n",
    "elif walk_type == 'Random':\n",
    "    neighbor_sample_func = Random_walk\n",
    "else:\n",
    "    neighbor_sample_func = None\n",
    "    print(\"The model will only takes the immediate neighbors.\")\n",
    "    #assert(False), \"unknown walk type (has to be one of the following: BFS, DFS, Random)\"\n",
    "\n",
    "def get_neighbors(ids, df, max_nodes, batch_size, traversal_func, max_branch_factor):\n",
    "    cols = []\n",
    "    rows = []\n",
    "    for idx, node_id in enumerate(ids):\n",
    "        nn_indices = traversal_func(df, node_id.item(), max_nodes, max_branch_factor)\n",
    "        col = [train_set.docid2index[v] for v in nn_indices]\n",
    "        rows += [idx] * len(col)\n",
    "        cols += col\n",
    "    data = [1] * len(cols)\n",
    "    connections = sparse.csr_matrix((data, (rows, cols)), shape=(batch_size, len(df)))\n",
    "    return torch.from_numpy(connections.toarray()).type(torch.FloatTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EdgeReg(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=10000, out_features=1000, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "    (3): ReLU(inplace)\n",
       "    (4): Dropout(p=0.1)\n",
       "  )\n",
       "  (h_to_mu): Linear(in_features=1000, out_features=32, bias=True)\n",
       "  (h_to_logvar): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=32, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=10000, bias=True)\n",
       "    (1): LogSoftmax()\n",
       "  )\n",
       "  (nn_decoder): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=10257, bias=True)\n",
       "    (1): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################################################################################################\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#######################################################################################################\n",
    "dataset_name = \"ng20\"\n",
    "\n",
    "train_set = TextAndNearestNeighborsDataset(dataset_name, 'dataset/clean', 'train')\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=100, shuffle=False)\n",
    "test_set = TextAndNearestNeighborsDataset(dataset_name, 'dataset/clean', 'test')\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=100, shuffle=False)\n",
    "\n",
    "#######################################################################################################\n",
    "y_dim = train_set.num_classes()\n",
    "num_bits = 32\n",
    "num_features = train_set[0][1].size(0)\n",
    "num_nodes = len(train_set)\n",
    "edge_weight = 10.0\n",
    "\n",
    "#######################################################################################################\n",
    "num_samples = 1\n",
    "\n",
    "if num_samples == 1:\n",
    "    model = EdgeReg(dataset_name, num_features, num_nodes, num_bits, dropoutProb=0.1, device=device)\n",
    "else:\n",
    "    print(\"number of samples (T) = {}\".format(args.num_samples))\n",
    "    model = EdgeReg_v2(dataset_name, num_features, num_nodes, num_bits, dropoutProb=0.1, device=device, T=args.num_samples)\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|██████████▉| 102/103 [00:15<00:00,  6.63it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (100) must match the size of tensor b (57) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7b9360ac04db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mnormalized_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mscore_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mscore_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (100) must match the size of tensor b (57) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "topn = 20\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "kl_weight = 0.\n",
    "kl_step = 1 / 5000.\n",
    "\n",
    "edge_weight = 0.\n",
    "edge_step = 1 / 1000.\n",
    "\n",
    "best_precision = 0\n",
    "best_precision_epoch = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = []\n",
    "    for ids, xb, yb, nb, scores in tqdm(train_loader, ncols=50):\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        cols = nb.view(-1)\n",
    "        cols = torch.LongTensor([train_set.docid2index[v.item()] for v in cols])\n",
    "        rows = torch.arange(0, nb.size(0)).unsqueeze(1).repeat(1, nb.size(1)).view(-1)\n",
    "        i = torch.stack([cols, rows])\n",
    "        v = torch.ones(cols.size(0))\n",
    "\n",
    "        nn_mat = torch.sparse.FloatTensor(i, v, torch.Size([num_nodes, nb.size(0)])).to_dense()\n",
    "        nn_mat = nn_mat.t().to(device)\n",
    "\n",
    "        denom = torch.max(scores, dim=1)[0] - torch.min(scores, dim=1)[0]\n",
    "        normalized_scores = (scores - torch.min(scores, dim=1)[0]) / denom\n",
    "        score_mat = torch.sparse.FloatTensor(i, normalized_scores.view(-1), torch.Size([num_nodes, nb.size(0)])).to_dense()\n",
    "        score_mat = score_mat.t().to(device)\n",
    "\n",
    "        logprob_w, logprob_nn, mu, logvar = model(xb)\n",
    "        kl_loss = EdgeReg.calculate_KL_loss(mu, logvar)\n",
    "        \n",
    "        if num_samples == 1:\n",
    "            reconstr_loss = EdgeReg.compute_reconstr_loss(logprob_w, xb)\n",
    "            nn_reconstr_loss = -(logprob_nn * nn_mat * score_mat).sum(dim=1).mean()\n",
    "        else:\n",
    "            reconstr_loss = EdgeReg_v2.compute_reconstr_loss(logprob_w, xb)\n",
    "            nn_reconstr_loss = -(logprob_nn * nn_mat * score_mat).sum(dim=1).mean()\n",
    "\n",
    "        loss = reconstr_loss + edge_weight * nn_reconstr_loss + kl_weight * kl_loss\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        kl_weight = min(kl_weight + kl_step, 1.)\n",
    "        edge_weight = min(edge_weight + edge_step, 1.)\n",
    "\n",
    "        avg_loss.append(loss.item())\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        train_b, test_b, train_y, test_y = model.get_binary_code(train_loader, test_loader)\n",
    "        retrieved_indices = retrieve_topk(test_b.to(device), train_b.to(device), topK=100)\n",
    "        prec = compute_precision_at_k(retrieved_indices, test_y.to(device), train_y.to(device), topK=100)\n",
    "        \n",
    "        print(\"loss: {:.4f}\".format(np.mean(avg_loss)))\n",
    "        print(\"precision at 100: {:.4f}\".format(prec.item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-528e020b9a7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_binary_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mretrieved_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_topk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_precision_at_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieved_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/schaidaroon/research/node2hash/model/EdgeReg.py\u001b[0m in \u001b[0;36mget_binary_code\u001b[0;34m(self, train, test)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_binary_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mtrain_zy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mtrain_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_zy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mtrain_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/home/schaidaroon/research/node2hash/model/EdgeReg.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_binary_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mtrain_zy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mtrain_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_zy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mtrain_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "train_b, test_b, train_y, test_y = model.get_binary_code(train_loader, test_loader)\n",
    "retrieved_indices = retrieve_topk(test_b.to(device), train_b.to(device), topK=100)\n",
    "prec = compute_precision_at_k(retrieved_indices, test_y.to(device), train_y.to(device), topK=100)\n",
    "\n",
    "print(\"loss: {:.4f}\".format(np.mean(avg_loss)))\n",
    "print(\"precision at 100: {:.4f}\".format(prec.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprob_nn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mat.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_pickle('dataset/clean/ng20/ng20.train.pkl')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.neighbors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research2018",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
