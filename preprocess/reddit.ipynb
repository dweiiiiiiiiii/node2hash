{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This script preprocess Reddit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotmap import DotMap\n",
    "import json\n",
    "from scipy import sparse\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "from tqdm import *\n",
    "\n",
    "dataset_name = 'reddit'\n",
    "data_path = os.path.join('../dataset/raw/{}'.format(dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(normalize=True):\n",
    "    graph_fn = os.path.join(data_path, '{}-G.json'.format(dataset_name))\n",
    "\n",
    "    print('load graph data ...')\n",
    "    G_data = json.load(open(graph_fn))\n",
    "    G = json_graph.node_link_graph(G_data)\n",
    "    if isinstance(G.nodes()[0], int):\n",
    "        conversion = lambda n : int(n)\n",
    "    else:\n",
    "        conversion = lambda n : n\n",
    "\n",
    "    print('load features, id map, and class map ...')\n",
    "    features_fn = os.path.join(data_path, '{}-feats.npy'.format(dataset_name))\n",
    "    feats = np.load(features_fn)\n",
    "        \n",
    "    id_map_fn = os.path.join(data_path, '{}-id_map.json'.format(dataset_name))\n",
    "    id_map = json.load(open(id_map_fn))\n",
    "    id_map = {k:int(v) for k,v in id_map.items()}\n",
    "    \n",
    "    class_fn = os.path.join(data_path, '{}-class_map.json'.format(dataset_name))\n",
    "    class_map = json.load(open(class_fn))\n",
    "    if isinstance(list(class_map.values())[0], list):\n",
    "        lab_conversion = lambda n : n\n",
    "    else:\n",
    "        lab_conversion = lambda n : int(n)\n",
    "\n",
    "    class_map = {k:lab_conversion(v) for k,v in class_map.items()}\n",
    "\n",
    "    ## Remove all nodes that do not have val/test annotations\n",
    "    ## (necessary because of networkx weirdness with the Reddit data)\n",
    "    broken_nodes = [node for node in G.nodes() if not 'val' in G.node[node] or not 'test' in G.node[node]]\n",
    "    G.remove_nodes_from(broken_nodes)\n",
    "    print(\"Removed {:d} nodes that lacked proper annotations due to networkx versioning issues\".format(len(broken_nodes)))\n",
    "\n",
    "    ## Make sure the graph has edge train_removed annotations\n",
    "    ## (some datasets might already have this..)\n",
    "    print(\"Loaded data.. now preprocessing..\")\n",
    "    for edge in G.edges():\n",
    "        if (G.node[edge[0]]['val'] or G.node[edge[1]]['val'] or\n",
    "            G.node[edge[0]]['test'] or G.node[edge[1]]['test']):\n",
    "            G[edge[0]][edge[1]]['train_removed'] = True\n",
    "        else:\n",
    "            G[edge[0]][edge[1]]['train_removed'] = False\n",
    "\n",
    "    if normalize and not feats is None:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        train_ids = np.array([id_map[n] for n in G.nodes() if not G.node[n]['val'] and not G.node[n]['test']])\n",
    "        train_feats = feats[train_ids]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_feats)\n",
    "        feats = scaler.transform(feats)\n",
    "\n",
    "    return G, feats, id_map, class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G, feats, id_map, class_map = load_data(normalize=False)\n",
    "print(feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = {}\n",
    "\n",
    "with open(os.path.join(data_path, 'reddit-adjlist.txt')) as in_fn:\n",
    "    for line in in_fn:\n",
    "        line = line.strip()\n",
    "        if line[0] == '#':\n",
    "            continue\n",
    "        \n",
    "        tokens = line.split()\n",
    "        node_id = tokens[0]\n",
    "        assert(node_id not in graphs)\n",
    "        \n",
    "        node = DotMap()\n",
    "        node.node_id = node_id\n",
    "        node.outgoing = tokens[1:]\n",
    "        node.incoming = []\n",
    "        graphs[node_id] = node\n",
    "        \n",
    "sink_nodes = {}\n",
    "for node_id in tqdm(graphs):\n",
    "    for out_node_id in graphs[node_id].outgoing:\n",
    "        if out_node_id in graphs:\n",
    "            graphs[out_node_id].incoming.append(node_id)\n",
    "        else:\n",
    "            if out_node_id not in sink_nodes:\n",
    "                node = DotMap()\n",
    "                node.node_id = out_node_id\n",
    "                node.incoming = [node_id]\n",
    "                node.outgoing = []\n",
    "                sink_nodes[out_node_id] = node\n",
    "            else:\n",
    "                sink_nodes[out_node_id].incoming.append(node_id)\n",
    "\n",
    "for node_id in sink_nodes:\n",
    "    graphs[node_id] = sink_nodes[node_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for split train-test-cv\n",
    "TRAIN_FLAG = 0\n",
    "TEST_FLAG = 1\n",
    "CV_FLAG = 2\n",
    "\n",
    "for node_id in G.nodes():\n",
    "    if node_id in graphs:\n",
    "        is_validate = G.node[node_id]['val']\n",
    "        is_test = G.node[node_id]['test']\n",
    "       \n",
    "        if is_test:\n",
    "            graphs[node_id].kind = TEST_FLAG\n",
    "        elif is_validate:\n",
    "            graphs[node_id].kind = CV_FLAG\n",
    "        else:\n",
    "            graphs[node_id].kind = TRAIN_FLAG\n",
    "            \n",
    "# add class labels\n",
    "for node_id, class_id in class_map.items():\n",
    "    if node_id in graphs:\n",
    "        graphs[node_id].class_id = class_id\n",
    "        \n",
    "# add node features\n",
    "for node_id, index in tqdm(id_map.items()):\n",
    "    if node_id in graphs:\n",
    "        graphs[node_id].features = list(feats[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = []\n",
    "for node_id, node in tqdm(graphs.items()):\n",
    "    # combine in and out edges\n",
    "    out_edges = list(set([id_map[n] for n in node.outgoing]))\n",
    "    in_edges = list(set([id_map[n] for n in node.incoming]))\n",
    "    neighbors = list(set(out_edges + in_edges))\n",
    "    \n",
    "    node_data = {'post_id': node.node_id,  \n",
    "                 'node_id': id_map[node.node_id],\n",
    "                 'neighbors': neighbors,\n",
    "                 'in_edges': in_edges, 'out_edges': out_edges,\n",
    "                 'label': node.class_id, 'kind': node.kind,\n",
    "                 'features': node.features}\n",
    "    \n",
    "    graph_data.append(node_data)\n",
    "\n",
    "df = pd.DataFrame(graph_data)\n",
    "df.set_index('node_id', inplace=True) # set paper as the row index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_path = os.path.join('../dataset/clean/{}'.format(dataset_name))\n",
    "save_fn = os.path.join(save_data_path, '{}.data.pkl'.format(dataset_name))\n",
    "df.to_pickle(save_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Graph Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_path = os.path.join('../dataset/clean/{}'.format(dataset_name))\n",
    "data_fn = os.path.join(save_data_path, '{}.data.pkl'.format(dataset_name))\n",
    "df = pd.from_pickle(load_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove any row that has no neighbors\n",
    "print(\"num nodes = {}\".format(len(df)))\n",
    "df = df[df.neighbors.apply(len) > 0]\n",
    "print(\"num nodes = {}\".format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df.kind == TRAIN_FLAG]\n",
    "df_test = df[df.kind == TEST_FLAG]\n",
    "df_cv = df[df.kind == CV_FLAG]\n",
    "\n",
    "print(\"num train: {} num test: {} num cv: {}\".format(len(df_train), \n",
    "                                                     len(df_test), \n",
    "                                                     len(df_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any non-train neighbors\n",
    "def remove_test_and_cv_edges(row):\n",
    "    return [r for r in row if r in df_train.index]\n",
    "\n",
    "df_train = df_train.copy()\n",
    "df_train.neighbors = df_train.neighbors.apply(remove_test_and_cv_edges)\n",
    "\n",
    "df_train = df_train[df_train.neighbors.apply(len) > 0]\n",
    "print(\"num trains: {}\".format(len(df_train)))\n",
    "\n",
    "# Remove any row that points to a removed train node\n",
    "df_train.neighbors = df_train.neighbors.apply(remove_test_and_cv_edges)\n",
    "df_train.neighbors.apply(len).describe()\n",
    "\n",
    "print(\"num trains: {}\".format(len(df_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Test and Validatation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"num test: {}\".format(len(df_test)))\n",
    "df_test = df_test.copy()\n",
    "df_test.neighbors = df_test.neighbors.apply(remove_test_and_cv_edges)\n",
    "df_test = df_test[df_test.neighbors.apply(len) > 0]\n",
    "print(\"num test: {}\".format(len(df_test)))\n",
    "\n",
    "print(\"num cv: {}\".format(len(df_cv)))\n",
    "df_cv = df_cv.copy()\n",
    "df_cv.neighbors = df_cv.neighbors.apply(remove_test_and_cv_edges)\n",
    "df_cv = df_cv[df_cv.neighbors.apply(len) > 0]\n",
    "print(\"num cv: {}\".format(len(df_cv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_id_2_train_id = {global_idx: idx for idx, global_idx \n",
    "                        in enumerate(df_train.index)}\n",
    "\n",
    "def convert_2_train_id(row):\n",
    "    return [global_id_2_train_id[r] for r in row]\n",
    "\n",
    "train_edges = df_train.neighbors.apply(convert_2_train_id)\n",
    "\n",
    "train_graph = {}\n",
    "for node_id, value in train_edges.iteritems():\n",
    "    train_graph[global_id_2_train_id[node_id]] = value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_data_path = os.path.join('../dataset/clean/{}'.format(dataset_name))\n",
    "save_fn = os.path.join(save_data_path, 'ind.{}.train.graph.pkl'.format(dataset_name))\n",
    "pickle.dump(train_graph, open(save_fn, 'wb'))\n",
    "print('save graph data to {}'.format(save_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_id_2_test_id = {global_idx: idx for idx, global_idx in enumerate(df_test.index)}\n",
    "\n",
    "# Convert each globalId to trainId because all test nodes only point to train nodes\n",
    "test_edges = df_test.neighbors.apply(convert_2_train_id) \n",
    "test_graph = {}\n",
    "for node_id, value in test_edges.iteritems():\n",
    "    test_graph[global_id_2_test_id[node_id]] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fn = os.path.join(save_data_path, 'ind.{}.test.graph.pkl'.format(dataset_name))\n",
    "pickle.dump(test_graph, open(save_fn, 'wb'))\n",
    "print('save graph data to {}'.format(save_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_id_2_cv_id = {global_idx: idx for idx, global_idx \n",
    "                        in enumerate(df_cv.index)}\n",
    "\n",
    "# Convert each globalId to trainId because all cv nodes only point to train nodes\n",
    "cv_edges = df_cv.neighbors.apply(convert_2_train_id) \n",
    "cv_graph = {}\n",
    "for node_id, value in cv_edges.iteritems():\n",
    "    cv_graph[global_id_2_cv_id[node_id]] = value\n",
    "    \n",
    "save_fn = os.path.join(save_data_path, 'ind.{}.cv.graph.pkl'.format(dataset_name))\n",
    "pickle.dump(test_graph, open(save_fn, 'wb'))\n",
    "print('save graph data to {}'.format(save_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Document features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = list(df_train.features)\n",
    "train_features = sparse.csr_matrix(train_features)\n",
    "train_labels = list(df_train.label)\n",
    "\n",
    "######################################################################################\n",
    "min_class_id = np.min(train_labels)\n",
    "max_class_id = np.max(train_labels)\n",
    "num_classes = max_class_id - min_class_id + 1\n",
    "\n",
    "gnd_train = sparse.csr_matrix(np.eye(num_classes)[train_labels])\n",
    "\n",
    "######################################################################################\n",
    "test_features = list(df_test.features)\n",
    "test_features = sparse.csr_matrix(test_features)\n",
    "\n",
    "test_labels = list(df_test.label)\n",
    "gnd_test = sparse.csr_matrix(np.eye(num_classes)[test_labels])\n",
    "\n",
    "######################################################################################\n",
    "cv_features = list(df_cv.features)\n",
    "cv_features = sparse.csr_matrix(cv_features)\n",
    "\n",
    "cv_labels = list(df_cv.label)\n",
    "gnd_cv = sparse.csr_matrix(np.eye(num_classes)[cv_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(train_features.shape[1] == test_features.shape[1] == cv_features.shape[1])\n",
    "assert(gnd_train.shape[1] == gnd_test.shape[1] == gnd_cv.shape[1])\n",
    "assert(train_features.shape[0] == gnd_train.shape[0])\n",
    "assert(test_features.shape[0] == gnd_test.shape[0])\n",
    "assert(cv_features.shape[0] == gnd_cv.shape[0])\n",
    "\n",
    "# data_path = os.path.join(os.environ['HOME'], 'projects/graph_embedding/graph_dataset', dataset_name)\n",
    "# save_fn = os.path.join(data_path, 'ind.{}.mat'.format(dataset_name))\n",
    "\n",
    "# scipy.io.savemat(save_fn, \n",
    "#                  mdict={'train': train_features, \n",
    "#                         'test': test_features, \n",
    "#                         'cv': cv_features,\n",
    "#                         'gnd_train': gnd_train, \n",
    "#                         'gnd_test': gnd_test,\n",
    "#                         'gnd_cv': gnd_cv})\n",
    "\n",
    "# print('save data to {}'.format(save_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "save_fn = os.path.join(save_data_path, 'ind.{}.mat'.format(dataset_name))\n",
    "scipy.io.savemat(save_fn, \n",
    "                 mdict={'train': train_features, \n",
    "                        'test': test_features, \n",
    "                        'cv': cv_features,\n",
    "                        'gnd_train': gnd_train, \n",
    "                        'gnd_test': gnd_test,\n",
    "                        'gnd_cv': gnd_cv})\n",
    "\n",
    "print('save data to {}'.format(save_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to dataframe with the format as doc_id, bow, label, and neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from scipy import sparse\n",
    "\n",
    "dataset_name = 'reddit'\n",
    "data_path = os.path.join('../dataset/clean/reddit')\n",
    "\n",
    "data = scipy.io.loadmat(os.path.join(data_path, 'ind.reddit.mat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = data['train']\n",
    "test_features = data['test']\n",
    "gnd_train = data['gnd_train']\n",
    "gnd_test = data['gnd_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_graph = pickle.load(open(os.path.join(data_path, \"ind.reddit.train.graph.pkl\"), \"rb\"))\n",
    "test_graph = pickle.load(open(os.path.join(data_path, \"ind.reddit.test.graph.pkl\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-cd7ac9be9f2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdoc_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mrow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_graph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcol\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_graph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_graph' is not defined"
     ]
    }
   ],
   "source": [
    "# create a connection matrix\n",
    "n_train = train_features.shape[0]\n",
    "row = []\n",
    "col = []\n",
    "for doc_id in train_graph:\n",
    "    row += [doc_id] * len(train_graph[doc_id])\n",
    "    col += train_graph[doc_id]\n",
    "data = [1] * len(row)\n",
    "train_connections = sparse.csr_matrix((data, (row, col)), shape=(n_train, n_train))\n",
    "\n",
    "n_test = test_features.shape[0]\n",
    "row = []\n",
    "col = []\n",
    "for doc_id in test_graph:\n",
    "    row += [doc_id] * len(test_graph[doc_id])\n",
    "    col += test_graph[doc_id]\n",
    "data = [1] * len(row)\n",
    "test_connections = sparse.csr_matrix((data, (row, col)), shape=(n_test, n_train)) # test graph points to train graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "save_dir = os.path.join('../dataset/clean', dataset_name)\n",
    "##########################################################################################\n",
    "\n",
    "train = []\n",
    "for doc_id in tqdm(train_graph):\n",
    "    doc = {'doc_id': doc_id, 'bow': train_feas[doc_id], \n",
    "           'label': gnd_train[doc_id], 'neighbors': train_connections[doc_id]}\n",
    "    train.append(doc)\n",
    "\n",
    "train_df = pd.DataFrame.from_dict(train)\n",
    "train_df.set_index('doc_id', inplace=True)\n",
    "\n",
    "fn = os.path.join(save_dir, '{}.train.pkl'.format(dataset_name))\n",
    "train_df.to_pickle(fn)\n",
    "##########################################################################################\n",
    "\n",
    "test = []\n",
    "for doc_id in tqdm(test_graph):\n",
    "    doc = {'doc_id': doc_id, 'bow': test_feas[doc_id], \n",
    "           'label': gnd_test[doc_id], 'neighbors': test_connections[doc_id]}\n",
    "    test.append(doc)\n",
    "\n",
    "test_df = pd.DataFrame.from_dict(test)\n",
    "test_df.set_index('doc_id', inplace=True)\n",
    "\n",
    "fn = os.path.join(save_dir, '{}.test.pkl'.format(dataset_name))\n",
    "test_df.to_pickle(fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research2018",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
