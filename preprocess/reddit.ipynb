{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This script preprocess Reddit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotmap import DotMap\n",
    "import json\n",
    "from scipy import sparse\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "from tqdm import *\n",
    "\n",
    "dataset_name = 'reddit'\n",
    "data_path = os.path.join('../dataset/raw/{}'.format(dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(normalize=True):\n",
    "    graph_fn = os.path.join(data_path, '{}-G.json'.format(dataset_name))\n",
    "\n",
    "    print('load graph data ...')\n",
    "    G_data = json.load(open(graph_fn))\n",
    "    G = json_graph.node_link_graph(G_data)\n",
    "    if isinstance(G.nodes()[0], int):\n",
    "        conversion = lambda n : int(n)\n",
    "    else:\n",
    "        conversion = lambda n : n\n",
    "\n",
    "    print('load features, id map, and class map ...')\n",
    "    features_fn = os.path.join(data_path, '{}-feats.npy'.format(dataset_name))\n",
    "    feats = np.load(features_fn)\n",
    "        \n",
    "    id_map_fn = os.path.join(data_path, '{}-id_map.json'.format(dataset_name))\n",
    "    id_map = json.load(open(id_map_fn))\n",
    "    id_map = {k:int(v) for k,v in id_map.items()}\n",
    "    \n",
    "    class_fn = os.path.join(data_path, '{}-class_map.json'.format(dataset_name))\n",
    "    class_map = json.load(open(class_fn))\n",
    "    if isinstance(list(class_map.values())[0], list):\n",
    "        lab_conversion = lambda n : n\n",
    "    else:\n",
    "        lab_conversion = lambda n : int(n)\n",
    "\n",
    "    class_map = {k:lab_conversion(v) for k,v in class_map.items()}\n",
    "\n",
    "    ## Remove all nodes that do not have val/test annotations\n",
    "    ## (necessary because of networkx weirdness with the Reddit data)\n",
    "    broken_nodes = [node for node in G.nodes() if not 'val' in G.node[node] or not 'test' in G.node[node]]\n",
    "    G.remove_nodes_from(broken_nodes)\n",
    "    print(\"Removed {:d} nodes that lacked proper annotations due to networkx versioning issues\".format(len(broken_nodes)))\n",
    "\n",
    "    ## Make sure the graph has edge train_removed annotations\n",
    "    ## (some datasets might already have this..)\n",
    "    print(\"Loaded data.. now preprocessing..\")\n",
    "    for edge in G.edges():\n",
    "        if (G.node[edge[0]]['val'] or G.node[edge[1]]['val'] or\n",
    "            G.node[edge[0]]['test'] or G.node[edge[1]]['test']):\n",
    "            G[edge[0]][edge[1]]['train_removed'] = True\n",
    "        else:\n",
    "            G[edge[0]][edge[1]]['train_removed'] = False\n",
    "\n",
    "    if normalize and not feats is None:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        train_ids = np.array([id_map[n] for n in G.nodes() if not G.node[n]['val'] and not G.node[n]['test']])\n",
    "        train_feats = feats[train_ids]\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train_feats)\n",
    "        feats = scaler.transform(feats)\n",
    "\n",
    "    return G, feats, id_map, class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load graph data ...\n",
      "load features, id map, and class map ...\n",
      "Removed 231443 nodes that lacked proper annotations due to networkx versioning issues\n",
      "Loaded data.. now preprocessing..\n",
      "(232965, 602)\n"
     ]
    }
   ],
   "source": [
    "G, feats, id_map, class_map = load_data(normalize=False)\n",
    "print(feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134999/134999 [02:10<00:00, 1034.12it/s]\n"
     ]
    }
   ],
   "source": [
    "graphs = {}\n",
    "\n",
    "with open(os.path.join(data_path, 'reddit-adjlist.txt')) as in_fn:\n",
    "    for line in in_fn:\n",
    "        line = line.strip()\n",
    "        if line[0] == '#':\n",
    "            continue\n",
    "        \n",
    "        tokens = line.split()\n",
    "        node_id = tokens[0]\n",
    "        assert(node_id not in graphs)\n",
    "        \n",
    "        node = DotMap()\n",
    "        node.node_id = node_id\n",
    "        node.outgoing = tokens[1:]\n",
    "        node.incoming = []\n",
    "        graphs[node_id] = node\n",
    "        \n",
    "sink_nodes = {}\n",
    "for node_id in tqdm(graphs):\n",
    "    for out_node_id in graphs[node_id].outgoing:\n",
    "        if out_node_id in graphs:\n",
    "            graphs[out_node_id].incoming.append(node_id)\n",
    "        else:\n",
    "            if out_node_id not in sink_nodes:\n",
    "                node = DotMap()\n",
    "                node.node_id = out_node_id\n",
    "                node.incoming = [node_id]\n",
    "                node.outgoing = []\n",
    "                sink_nodes[out_node_id] = node\n",
    "            else:\n",
    "                sink_nodes[out_node_id].incoming.append(node_id)\n",
    "\n",
    "for node_id in sink_nodes:\n",
    "    graphs[node_id] = sink_nodes[node_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232965/232965 [00:18<00:00, 12449.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# for split train-test-cv\n",
    "TRAIN_FLAG = 0\n",
    "TEST_FLAG = 1\n",
    "CV_FLAG = 2\n",
    "\n",
    "for node_id in G.nodes():\n",
    "    if node_id in graphs:\n",
    "        is_validate = G.node[node_id]['val']\n",
    "        is_test = G.node[node_id]['test']\n",
    "       \n",
    "        if is_test:\n",
    "            graphs[node_id].kind = TEST_FLAG\n",
    "        elif is_validate:\n",
    "            graphs[node_id].kind = CV_FLAG\n",
    "        else:\n",
    "            graphs[node_id].kind = TRAIN_FLAG\n",
    "            \n",
    "# add class labels\n",
    "for node_id, class_id in class_map.items():\n",
    "    if node_id in graphs:\n",
    "        graphs[node_id].class_id = class_id\n",
    "        \n",
    "# add node features\n",
    "for node_id, index in tqdm(id_map.items()):\n",
    "    if node_id in graphs:\n",
    "        graphs[node_id].features = list(feats[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232383/232383 [01:11<00:00, 3229.77it/s]\n"
     ]
    }
   ],
   "source": [
    "graph_data = []\n",
    "for node_id, node in tqdm(graphs.items()):\n",
    "    # combine in and out edges\n",
    "    out_edges = list(set([id_map[n] for n in node.outgoing]))\n",
    "    in_edges = list(set([id_map[n] for n in node.incoming]))\n",
    "    neighbors = list(set(out_edges + in_edges))\n",
    "    \n",
    "    node_data = {'post_id': node.node_id,  \n",
    "                 'node_id': id_map[node.node_id],\n",
    "                 'neighbors': neighbors,\n",
    "                 'in_edges': in_edges, 'out_edges': out_edges,\n",
    "                 'label': node.class_id, 'kind': node.kind,\n",
    "                 'features': node.features}\n",
    "    \n",
    "    graph_data.append(node_data)\n",
    "\n",
    "df = pd.DataFrame(graph_data)\n",
    "df.set_index('node_id', inplace=True) # set paper as the row index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_path = os.path.join('../dataset/clean/{}'.format(dataset_name))\n",
    "save_fn = os.path.join(save_data_path, '{}.data.pkl'.format(dataset_name))\n",
    "df.to_pickle(save_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Graph Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_path = os.path.join('../dataset/clean/{}'.format(dataset_name))\n",
    "data_fn = os.path.join(save_data_path, '{}.data.pkl'.format(dataset_name))\n",
    "df = pd.from_pickle(load_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num nodes = 232383\n",
      "num nodes = 232383\n"
     ]
    }
   ],
   "source": [
    "# We remove any row that has no neighbors\n",
    "print(\"num nodes = {}\".format(len(df)))\n",
    "df = df[df.neighbors.apply(len) > 0]\n",
    "print(\"num nodes = {}\".format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train: 152157 num test: 55228 num cv: 23660\n"
     ]
    }
   ],
   "source": [
    "df_train = df[df.kind == TRAIN_FLAG]\n",
    "df_test = df[df.kind == TEST_FLAG]\n",
    "df_cv = df[df.kind == CV_FLAG]\n",
    "\n",
    "print(\"num train: {} num test: {} num cv: {}\".format(len(df_train), \n",
    "                                                     len(df_test), \n",
    "                                                     len(df_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num trains: 151741\n",
      "num trains: 151741\n"
     ]
    }
   ],
   "source": [
    "# Remove any non-train neighbors\n",
    "def remove_test_and_cv_edges(row):\n",
    "    return [r for r in row if r in df_train.index]\n",
    "\n",
    "df_train = df_train.copy()\n",
    "df_train.neighbors = df_train.neighbors.apply(remove_test_and_cv_edges)\n",
    "\n",
    "df_train = df_train[df_train.neighbors.apply(len) > 0]\n",
    "print(\"num trains: {}\".format(len(df_train)))\n",
    "\n",
    "# Remove any row that points to a removed train node\n",
    "df_train.neighbors = df_train.neighbors.apply(remove_test_and_cv_edges)\n",
    "df_train.neighbors.apply(len).describe()\n",
    "\n",
    "print(\"num trains: {}\".format(len(df_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Test and Validatation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num test: 55228\n",
      "num test: 53736\n",
      "num cv: 23660\n",
      "num cv: 23012\n"
     ]
    }
   ],
   "source": [
    "print(\"num test: {}\".format(len(df_test)))\n",
    "df_test = df_test.copy()\n",
    "df_test.neighbors = df_test.neighbors.apply(remove_test_and_cv_edges)\n",
    "df_test = df_test[df_test.neighbors.apply(len) > 0]\n",
    "print(\"num test: {}\".format(len(df_test)))\n",
    "\n",
    "print(\"num cv: {}\".format(len(df_cv)))\n",
    "df_cv = df_cv.copy()\n",
    "df_cv.neighbors = df_cv.neighbors.apply(remove_test_and_cv_edges)\n",
    "df_cv = df_cv[df_cv.neighbors.apply(len) > 0]\n",
    "print(\"num cv: {}\".format(len(df_cv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_id_2_train_id = {global_idx: idx for idx, global_idx \n",
    "                        in enumerate(df_train.index)}\n",
    "\n",
    "def convert_2_train_id(row):\n",
    "    return [global_id_2_train_id[r] for r in row]\n",
    "\n",
    "train_edges = df_train.neighbors.apply(convert_2_train_id)\n",
    "\n",
    "train_graph = {}\n",
    "for node_id, value in train_edges.iteritems():\n",
    "    train_graph[global_id_2_train_id[node_id]] = value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save graph data to ../dataset/clean/reddit/int.reddit.train.graph.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "save_data_path = os.path.join('../dataset/clean/{}'.format(dataset_name))\n",
    "save_fn = os.path.join(save_data_path, 'int.{}.train.graph.pkl'.format(dataset_name))\n",
    "pickle.dump(train_graph, open(save_fn, 'wb'))\n",
    "print('save graph data to {}'.format(save_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_id_2_test_id = {global_idx: idx for idx, global_idx in enumerate(df_test.index)}\n",
    "\n",
    "# Convert each globalId to trainId because all test nodes only point to train nodes\n",
    "test_edges = df_test.neighbors.apply(convert_2_train_id) \n",
    "test_graph = {}\n",
    "for node_id, value in test_edges.iteritems():\n",
    "    test_graph[global_id_2_test_id[node_id]] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save graph data to ../dataset/clean/reddit/ind.reddit.test.graph.pkl\n"
     ]
    }
   ],
   "source": [
    "save_fn = os.path.join(save_data_path, 'ind.{}.test.graph.pkl'.format(dataset_name))\n",
    "pickle.dump(test_graph, open(save_fn, 'wb'))\n",
    "print('save graph data to {}'.format(save_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save graph data to ../dataset/clean/reddit/ind.reddit.cv.graph.pkl\n"
     ]
    }
   ],
   "source": [
    "global_id_2_cv_id = {global_idx: idx for idx, global_idx \n",
    "                        in enumerate(df_cv.index)}\n",
    "\n",
    "# Convert each globalId to trainId because all cv nodes only point to train nodes\n",
    "cv_edges = df_cv.neighbors.apply(convert_2_train_id) \n",
    "cv_graph = {}\n",
    "for node_id, value in cv_edges.iteritems():\n",
    "    cv_graph[global_id_2_cv_id[node_id]] = value\n",
    "    \n",
    "save_fn = os.path.join(save_data_path, 'ind.{}.cv.graph.pkl'.format(dataset_name))\n",
    "pickle.dump(test_graph, open(save_fn, 'wb'))\n",
    "print('save graph data to {}'.format(save_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Document features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = list(df_train.features)\n",
    "train_features = sparse.csr_matrix(train_features)\n",
    "train_labels = list(df_train.label)\n",
    "\n",
    "######################################################################################\n",
    "min_class_id = np.min(train_labels)\n",
    "max_class_id = np.max(train_labels)\n",
    "num_classes = max_class_id - min_class_id + 1\n",
    "\n",
    "gnd_train = sparse.csr_matrix(np.eye(num_classes)[train_labels])\n",
    "\n",
    "######################################################################################\n",
    "test_features = list(df_test.features)\n",
    "test_features = sparse.csr_matrix(test_features)\n",
    "\n",
    "test_labels = list(df_test.label)\n",
    "gnd_test = sparse.csr_matrix(np.eye(num_classes)[test_labels])\n",
    "\n",
    "######################################################################################\n",
    "cv_features = list(df_cv.features)\n",
    "cv_features = sparse.csr_matrix(cv_features)\n",
    "\n",
    "cv_labels = list(df_cv.label)\n",
    "gnd_cv = sparse.csr_matrix(np.eye(num_classes)[cv_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(train_features.shape[1] == test_features.shape[1] == cv_features.shape[1])\n",
    "assert(gnd_train.shape[1] == gnd_test.shape[1] == gnd_cv.shape[1])\n",
    "assert(train_features.shape[0] == gnd_train.shape[0])\n",
    "assert(test_features.shape[0] == gnd_test.shape[0])\n",
    "assert(cv_features.shape[0] == gnd_cv.shape[0])\n",
    "\n",
    "# data_path = os.path.join(os.environ['HOME'], 'projects/graph_embedding/graph_dataset', dataset_name)\n",
    "# save_fn = os.path.join(data_path, 'ind.{}.mat'.format(dataset_name))\n",
    "\n",
    "# scipy.io.savemat(save_fn, \n",
    "#                  mdict={'train': train_features, \n",
    "#                         'test': test_features, \n",
    "#                         'cv': cv_features,\n",
    "#                         'gnd_train': gnd_train, \n",
    "#                         'gnd_test': gnd_test,\n",
    "#                         'gnd_cv': gnd_cv})\n",
    "\n",
    "# print('save data to {}'.format(save_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save data to ../dataset/clean/reddit/ind.reddit.mat\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "\n",
    "save_fn = os.path.join(save_data_path, 'ind.{}.mat'.format(dataset_name))\n",
    "scipy.io.savemat(save_fn, \n",
    "                 mdict={'train': train_features, \n",
    "                        'test': test_features, \n",
    "                        'cv': cv_features,\n",
    "                        'gnd_train': gnd_train, \n",
    "                        'gnd_test': gnd_test,\n",
    "                        'gnd_cv': gnd_cv})\n",
    "\n",
    "print('save data to {}'.format(save_fn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research2018",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
