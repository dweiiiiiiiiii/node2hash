{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "\n",
    "dataset_name = 'pubmed'\n",
    "data_dir = os.path.join('../datasets/raw', dataset_name)\n",
    "fn = os.path.join(data_dir, 'ind.{}.mat'.format(dataset_name))\n",
    "data = scipy.io.loadmat(fn)\n",
    "\n",
    "n_train = data['all_x'].shape[0]\n",
    "n_test = data['tx'].shape[0]\n",
    "test_indices = np.squeeze(data['test_idx'])\n",
    "\n",
    "print('num train: {} num test: {}'.format(n_train, n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testIds = set(test_indices)\n",
    "\n",
    "#Remove any test node that only links to the test nodes\n",
    "def load_graph(fn):\n",
    "    graph = {}\n",
    "\n",
    "    with open(fn) as in_csv:\n",
    "        for line in in_csv:\n",
    "            tokens = line.strip().split(',')\n",
    "            nodeIDs = [int(t) for t in tokens]\n",
    "            key = nodeIDs[0]\n",
    "            neighbors = nodeIDs[1:]\n",
    "            graph[key] = neighbors\n",
    "    return graph\n",
    "\n",
    "graph_fn = os.path.join(data_dir, 'ind.{}.graph.csv'.format(dataset_name))\n",
    "gp = load_graph(graph_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph should contain the same number of train data as all_x\n",
    "train_indices = [key for key in gp if key not in testIds and key < n_train]\n",
    "assert(len(train_indices) == n_train)\n",
    "\n",
    "# get valid train indices\n",
    "train_indices = [key for key in gp if key not in testIds]\n",
    "extra_train_indices = [idx for idx in train_indices if idx >= n_train]\n",
    "#print(len(train_indices))\n",
    "#print(len(extra_train_indices))\n",
    "train_indices = [idx for idx in train_indices if idx not in extra_train_indices]\n",
    "#print(len(train_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "valid_train_nodes = []\n",
    "for nodeId in train_indices:    \n",
    "    vertices = gp[nodeId]\n",
    "    num_nodes = len(vertices)\n",
    "    num_test_nodes = len([v for v in vertices if v in testIds or v in extra_train_indices])\n",
    "    \n",
    "    if num_nodes > num_test_nodes:\n",
    "        valid_train_nodes.append(nodeId)\n",
    "        \n",
    "print('total train: {}'.format(n_train))\n",
    "print('total valid train: {}'.format(len(valid_train_nodes)))\n",
    "\n",
    "# figure out the trainId to keep (starting from 0)\n",
    "valid_train_ids = [idx for idx, trainId in enumerate(train_indices) if trainId in valid_train_nodes]\n",
    "assert(len(valid_train_nodes) == len(valid_train_ids))\n",
    "\n",
    "# filter the test data and labels\n",
    "train_data = data['all_x'][valid_train_ids, :]\n",
    "train_labels = data['all_y'][valid_train_ids, :]\n",
    "assert(train_data.shape[0] == train_labels.shape[0] == len(valid_train_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "valid_test_nodes = []\n",
    "for testId in test_indices:\n",
    "    vertices = gp[testId]\n",
    "    \n",
    "    num_nodes = len(vertices)\n",
    "    num_valid_train_nodes = len([v for v in vertices if v in set(valid_train_ids)])\n",
    "    \n",
    "    if num_valid_train_nodes > 0:\n",
    "        valid_test_nodes.append(testId)\n",
    "        \n",
    "print('total test: {}'.format(n_test))\n",
    "print('total valid test: {}'.format(len(valid_test_nodes)))\n",
    "\n",
    "# make sure there is no duplication\n",
    "assert(len(valid_test_nodes) == len(set(valid_test_nodes)))\n",
    "\n",
    "# figure out the testId to keep (starting from 0)\n",
    "valid_test_ids = [idx for idx, testId in enumerate(test_indices) if testId in valid_test_nodes]\n",
    "assert(len(valid_test_nodes) == len(valid_test_ids))\n",
    "\n",
    "# filter the test data and labels\n",
    "test_data = data['tx'][valid_test_ids, :]\n",
    "test_labels = data['ty'][valid_test_ids, :]\n",
    "assert(test_data.shape[0] == test_labels.shape[0] == len(valid_test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a conversion from global id to trainId\n",
    "globalId2TrainID = {}\n",
    "for trainId, globalId in enumerate(valid_train_nodes):\n",
    "    globalId2TrainID[globalId] = trainId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "# create train graph\n",
    "train_graph = {}\n",
    "for nodeId in tqdm(valid_train_nodes):\n",
    "    assert(nodeId in gp)\n",
    "    vertices = [globalId2TrainID[v] for v in gp[nodeId] if v in set(valid_train_nodes)]\n",
    "    \n",
    "    assert(len(vertices) > 0)\n",
    "    train_graph[globalId2TrainID[nodeId]] = vertices\n",
    "    \n",
    "# create test graph\n",
    "test_graph = {}\n",
    "for testId, nodeId in enumerate(valid_test_nodes):\n",
    "    assert(nodeId in gp)\n",
    "    vertices = [globalId2TrainID[v] for v in gp[nodeId] if v in set(valid_train_nodes)]\n",
    "    \n",
    "    assert(len(vertices) > 0)\n",
    "    test_graph[testId] = vertices # use index starting from 0\n",
    "    \n",
    "assert(len(train_graph) == train_data.shape[0])\n",
    "assert(len(test_graph) == test_data.shape[0])\n",
    "\n",
    "print('train: {} test: {}'.format(train_data.shape[0], test_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels to a sparse matrix format\n",
    "import sklearn.preprocessing\n",
    "from scipy import sparse\n",
    "\n",
    "train_labels = np.argmax(train_labels, axis=1)\n",
    "test_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "n_classes = np.max(train_labels) - np.min(train_labels) + 1\n",
    "\n",
    "label_binarizer = sklearn.preprocessing.LabelBinarizer()\n",
    "label_binarizer.fit(range(n_classes))\n",
    "\n",
    "gnd_train = label_binarizer.transform(train_labels)\n",
    "gnd_test = label_binarizer.transform(test_labels)\n",
    "gnd_train = sparse.csr_matrix(gnd_train)\n",
    "gnd_test = sparse.csr_matrix(gnd_test)\n",
    "\n",
    "print(gnd_train.shape)\n",
    "print(gnd_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a connection matrix\n",
    "n_train = train_data.shape[0]\n",
    "train_connections = np.zeros((n_train, n_train), dtype=int)\n",
    "for doc_id in train_graph:\n",
    "    train_connections[doc_id][train_graph[doc_id]] = 1\n",
    "train_connections = sparse.csr_matrix(train_connections)\n",
    "\n",
    "n_test = test_data.shape[0]\n",
    "test_connections = np.zeros((n_test, n_train), dtype=int)\n",
    "for doc_id in test_graph:\n",
    "    test_connections[doc_id][test_graph[doc_id]] = 1\n",
    "test_connections = sparse.csr_matrix(test_connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join('../datasets/clean', dataset_name)\n",
    "##########################################################################################\n",
    "\n",
    "train = []\n",
    "for doc_id in train_graph:\n",
    "    doc = {'doc_id': doc_id, 'bow': train_data[doc_id], \n",
    "           'label': gnd_train[doc_id], 'neighbors': train_connections[doc_id]}\n",
    "    train.append(doc)\n",
    "\n",
    "train_df = pd.DataFrame.from_dict(train)\n",
    "train_df.set_index('doc_id', inplace=True)\n",
    "\n",
    "fn = os.path.join(save_dir, '{}.train.pkl'.format(dataset_name))\n",
    "train_df.to_pickle(fn)\n",
    "##########################################################################################\n",
    "\n",
    "test = []\n",
    "for doc_id in test_graph:\n",
    "    doc = {'doc_id': doc_id, 'bow': test_data[doc_id], \n",
    "           'label': gnd_test[doc_id], 'neighbors': test_connections[doc_id]}\n",
    "    test.append(doc)\n",
    "\n",
    "test_df = pd.DataFrame.from_dict(test)\n",
    "test_df.set_index('doc_id', inplace=True)\n",
    "\n",
    "fn = os.path.join(save_dir, '{}.test.pkl'.format(dataset_name))\n",
    "test_df.to_pickle(fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research2018",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
