{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import scipy.io\n",
    "\n",
    "dataset_name = 'dblp'\n",
    "data_path = os.path.join('../dataset/raw/{}'.format(dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = []\n",
    "incomming = {}\n",
    "\n",
    "for i in range(4):\n",
    "    fn = os.path.join(data_path, 'dblp-ref-{}.json'.format(i))\n",
    "    with open(fn) as in_fn:\n",
    "        for line in in_fn:\n",
    "            paper = json.loads(line.strip())\n",
    "            citations.append(paper)\n",
    "\n",
    "            if 'references' in paper:\n",
    "                for ref_id in paper['references']:\n",
    "                    if ref_id in incomming:\n",
    "                        incomming[ref_id].append(paper['id'])\n",
    "                    else:\n",
    "                        incomming[ref_id] = [paper['id']]\n",
    "                        \n",
    "df = pd.DataFrame(citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_first_line = True\n",
    "conferences = {}\n",
    "with open('../dataset/clean/dblp/venue_info.tsv') as in_csv:\n",
    "    for line in in_csv:\n",
    "        tokens = line.strip().split('\\t')\n",
    "        if is_first_line:\n",
    "            #print(tokens)\n",
    "            is_first_line = False\n",
    "        else:\n",
    "            conf_name = tokens[0]\n",
    "        \n",
    "            labels = [int(num_str) for num_str in tokens[2].split(',')]\n",
    "            labels = [n-2 for n in labels if n > 1] # remove the first label (signal processing has too many documents)\n",
    "            \n",
    "            conferences[conf_name] = {'name': conf_name, 'label': labels}\n",
    "        #conferences[conf_name] = {'name': conf_name, }\n",
    "\n",
    "max_labels = np.max([np.max(val['label']) for key, val in conferences.items()])\n",
    "min_labels = np.min([np.min(val['label']) for key, val in conferences.items()])\n",
    "num_labels = max_labels - min_labels + 1\n",
    "print('label min:{} max:{} total:{}'.format(min_labels, max_labels, num_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any row that is not present in the selected venues\n",
    "def is_selected_venue(row):\n",
    "    return (row in conferences)\n",
    "\n",
    "print(\"num paper (before): {}\".format(len(df)))\n",
    "df = df[df.venue.apply(is_selected_venue)]\n",
    "print(\"num paper (after): {}\".format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off_years = 2016\n",
    "\n",
    "df_train = df[df.year < cut_off_years]\n",
    "df_test = df[df.year >= cut_off_years]\n",
    "num_trains = len(df_train)\n",
    "num_tests = len(df_test)\n",
    "print(\"num trains: {} num tests: {} ratio: {:.4f}\".format(num_trains, num_tests, num_tests / num_trains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#venue_count = df_train.groupby('venue').count().sort_values(['abstract'], ascending=False).abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels(venue):\n",
    "    label_list = conferences[venue]['label']\n",
    "    return np.sum(np.eye(num_labels)[label_list], axis=0).astype(np.int)\n",
    "\n",
    "df_train = df_train.copy()\n",
    "df_train['label'] = df_train.venue.apply(assign_labels)\n",
    "df_train.set_index('id', inplace=True) # set paper as the row index\n",
    "\n",
    "df_test = df_test.copy()\n",
    "df_test['label'] = df_test.venue.apply(assign_labels)\n",
    "df_test.set_index('id', inplace=True) # set paper as the row index\n",
    "\n",
    "num_train_doc_per_labels = np.sum(np.array(list(df_train.label)), axis=0)\n",
    "num_test_doc_per_labels = np.sum(np.array(list(df_test.label)), axis=0)\n",
    "print(num_train_doc_per_labels)\n",
    "print(num_test_doc_per_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any row that does not have abstract, title, paperId, or venue\n",
    "print(\"num paper = {}\".format(len(df_train)))\n",
    "df_train.dropna(axis=0, subset=['abstract', 'venue', 'year', 'label'], inplace=True)\n",
    "print(\"num paper = {}\".format(len(df_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method adds incoming edges to each node as well as removing any edge that points outside the train set\n",
    "def createEdges(row):\n",
    "    if row.references is not np.nan:\n",
    "        outgoing_edges = [r for r in row.references if r in df_train.index]\n",
    "    else:\n",
    "        outgoing_edges = []\n",
    "        \n",
    "    if row.name in incomming:\n",
    "        incomming_edges = [r for r in incomming[row.name] if r in df_train.index]\n",
    "    else:\n",
    "        incomming_edges = []\n",
    "    return outgoing_edges + incomming_edges\n",
    "    \n",
    "df_train['links'] = df_train.apply(createEdges, axis=1)\n",
    "\n",
    "# Remove any row that has no link\n",
    "print(\"num paper = {}\".format(len(df_train)))\n",
    "df_train = df_train[df_train.links.apply(len) > 0]\n",
    "print(\"num paper = {}\".format(len(df_train)))\n",
    "\n",
    "# There must be no train nodes that references to non-train nodes\n",
    "def count_invalid_edges(refs):\n",
    "    return len([r for r in refs if r not in df_train.index])\n",
    "    \n",
    "assert(len(df_train[df_train.links.apply(count_invalid_edges) > 0]) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_id_2_train_id = {node_id: idx for idx, node_id in enumerate(df_train.index)}\n",
    "\n",
    "def convert_2_train_id(ref):\n",
    "    return [global_id_2_train_id[r] for r in ref]\n",
    "\n",
    "train_edges = df_train.links.apply(convert_2_train_id)\n",
    "   \n",
    "train_graph = {}\n",
    "for node_id, value in train_edges.iteritems():\n",
    "    train_graph[global_id_2_train_id[node_id]] = value\n",
    "    \n",
    "print('num train: {}'.format(len(train_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any row that does not have abstract, title, paperId, or venue\n",
    "print(\"num paper = {}\".format(len(df_test)))\n",
    "df_test.dropna(axis=0, subset=['abstract', 'venue', 'year', 'label'], inplace=True)\n",
    "print(\"num paper = {}\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method adds incoming edges to each node as well as removing any edge that points outside the train set\n",
    "def createEdges(row):\n",
    "    if row.references is not np.nan:\n",
    "        outgoing_edges = [r for r in row.references if r in df_train.index]\n",
    "    else:\n",
    "        outgoing_edges = []\n",
    "        \n",
    "    if row.name in incomming:\n",
    "        incomming_edges = [r for r in incomming[row.name] if r in df_train.index]\n",
    "    else:\n",
    "        incomming_edges = []\n",
    "    return outgoing_edges + incomming_edges\n",
    "    \n",
    "df_test['links'] = df_test.apply(createEdges, axis=1)\n",
    "\n",
    "# Remove any row that has no link\n",
    "print(\"num paper = {}\".format(len(df_test)))\n",
    "df_test = df_test[df_test.links.apply(len) > 0]\n",
    "print(\"num paper = {}\".format(len(df_test)))\n",
    "\n",
    "# There must be no train nodes that references to non-train nodes\n",
    "def count_invalid_edges(refs):\n",
    "    return len([r for r in refs if r not in df_train.index])\n",
    "    \n",
    "assert(len(df_test[df_test.links.apply(count_invalid_edges) > 0]) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_id_2_test_id = {node_id: idx for idx, node_id in enumerate(df_test.index)}\n",
    "\n",
    "# each link MUST point to the train nodes\n",
    "test_edges = df_test.links.apply(convert_2_train_id)\n",
    "   \n",
    "test_graph = {}\n",
    "for node_id, value in test_edges.iteritems():\n",
    "    test_graph[global_id_2_test_id[node_id]] = value\n",
    "    \n",
    "print('num test: {}'.format(len(test_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../dataset/clean/dblp'\n",
    "save_fn = os.path.join(data_path, 'ind.{}.train.graph.pk'.format(dataset_name))\n",
    "pickle.dump(train_graph, open(save_fn, 'wb'))\n",
    "print('save graph data to {}'.format(save_fn))\n",
    "\n",
    "save_fn = os.path.join(data_path, 'ind.{}.test.graph.pk'.format(dataset_name))\n",
    "pickle.dump(test_graph, open(save_fn, 'wb'))\n",
    "print('save graph data to {}'.format(save_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.8, min_df=5, sublinear_tf=True, max_features=10000)\n",
    "\n",
    "train_feas = vectorizer.fit_transform(list(df_train.abstract))\n",
    "print(np.nonzero(np.sum(train_feas, axis=1))[0].shape)\n",
    "\n",
    "test_feas = vectorizer.transform(list(df_test.abstract))\n",
    "print(np.nonzero(np.sum(test_feas, axis=1))[0].shape)\n",
    "\n",
    "gnd_train = sparse.csr_matrix(np.array(list(df_train.label)))\n",
    "gnd_test = sparse.csr_matrix(np.array(list(df_test.label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert(train_feas.shape[1] == test_feas.shape[1])\n",
    "assert(gnd_train.shape[1] == gnd_test.shape[1])\n",
    "assert(train_feas.shape[0] == gnd_train.shape[0])\n",
    "assert(test_feas.shape[0] == gnd_test.shape[0])\n",
    "\n",
    "data_path = '../dataset/clean/dblp'\n",
    "save_fn = os.path.join(data_path, 'ind.{}.mat'.format(dataset_name))\n",
    "\n",
    "scipy.io.savemat(save_fn, \n",
    "                 mdict={'train': train_feas, \n",
    "                        'test': test_feas, \n",
    "                        'cv': test_feas,\n",
    "                        'gnd_train': gnd_train, \n",
    "                        'gnd_test': gnd_test,\n",
    "                        'gnd_cv': gnd_test})\n",
    "\n",
    "print('save data to {}'.format(save_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to dataframe with the format as doc_id, bow, label, and neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a connection matrix\n",
    "n_train = train_feas.shape[0]\n",
    "row = []\n",
    "col = []\n",
    "for doc_id in train_graph:\n",
    "    row += [doc_id] * len(train_graph[doc_id])\n",
    "    col += train_graph[doc_id]\n",
    "data = [1] * len(row)\n",
    "train_connections = sparse.csr_matrix((data, (row, col)), shape=(n_train, n_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = test_feas.shape[0]\n",
    "row = []\n",
    "col = []\n",
    "for doc_id in test_graph:\n",
    "    row += [doc_id] * len(test_graph[doc_id])\n",
    "    col += test_graph[doc_id]\n",
    "data = [1] * len(row)\n",
    "test_connections = sparse.csr_matrix((data, (row, col)), shape=(n_test, n_train)) # test graph points to train graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "save_dir = os.path.join('../dataset/clean', dataset_name)\n",
    "##########################################################################################\n",
    "\n",
    "train = []\n",
    "for doc_id in tqdm(train_graph):\n",
    "    doc = {'doc_id': doc_id, 'bow': train_feas[doc_id], \n",
    "           'label': gnd_train[doc_id], 'neighbors': train_connections[doc_id]}\n",
    "    train.append(doc)\n",
    "\n",
    "train_df = pd.DataFrame.from_dict(train)\n",
    "train_df.set_index('doc_id', inplace=True)\n",
    "\n",
    "fn = os.path.join(save_dir, '{}.train.pkl'.format(dataset_name))\n",
    "train_df.to_pickle(fn)\n",
    "##########################################################################################\n",
    "\n",
    "test = []\n",
    "for doc_id in tqdm(test_graph):\n",
    "    doc = {'doc_id': doc_id, 'bow': test_feas[doc_id], \n",
    "           'label': gnd_test[doc_id], 'neighbors': test_connections[doc_id]}\n",
    "    test.append(doc)\n",
    "\n",
    "test_df = pd.DataFrame.from_dict(test)\n",
    "test_df.set_index('doc_id', inplace=True)\n",
    "\n",
    "fn = os.path.join(save_dir, '{}.test.pkl'.format(dataset_name))\n",
    "test_df.to_pickle(fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research2018",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
